{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"premium","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **CAP 5404 Deep Learning for Computer Graphics**\n","# *Project II. Neural Networks & Computer Graphics*\n","\n","Pranath Reddy Kumbam (**UFID**: 8512-0977)\n","\n"],"metadata":{"id":"AaRDEY9H_DML"}},{"cell_type":"markdown","source":["## Part 2: Regression"],"metadata":{"id":"CaXfY7MLtEoU"}},{"cell_type":"markdown","source":["### Load Datasets"],"metadata":{"id":"-cBFUu3tAUwc"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"hFiKBOt4WYz0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Path to Working Directory \n","%cd drive/My Drive/Acad/DLCG/Project2"],"metadata":{"id":"wcQxFN87WdEM"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ewNwonrJpRVX"},"outputs":[],"source":["# Import libraries\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import os\n","import random\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","from sklearn.utils import shuffle\n","from torchvision import models\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","source":["# Mean Chrominance values\n","def get_mean_chrominance(img1, img2):\n","  return (np.mean(img1), np.mean(img2))\n","\n","# Min-Max Norm \n","def norm(images):\n","    data = []\n","    for sample in images:\n","      min = np.amin(sample)\n","      max = np.amax(sample)\n","      range_val = max - min\n","      sample = (sample-min)/range_val\n","      sample = sample.reshape(1,128,128)\n","      data.append(sample)\n","    data = np.asarray(data)\n","    return data\n","\n","# Import Data\n","l_train = norm(np.load('./Data/arrays/Faces/L_train.npy'))\n","a_train = np.load('./Data/arrays/Faces/a_train.npy')/255\n","b_train = np.load('./Data/arrays/Faces/b_train.npy')/255\n","\n","l_test = norm(np.load('./Data/arrays/Faces/L_test.npy'))\n","a_test = np.load('./Data/arrays/Faces/a_test.npy')/255\n","b_test = np.load('./Data/arrays/Faces/b_test.npy')/255\n","\n","x_train = l_train\n","y_train = np.array([get_mean_chrominance(a_train[x], b_train[x]) for x in range(x_train.shape[0])])\n","y_ts = np.array([get_mean_chrominance(a_test[x], b_test[x]) for x in range(l_test.shape[0])])\n","\n","x_val = l_test[int(l_test.shape[0]*0.5):]\n","y_val = y_ts[int(l_test.shape[0]*0.5):]\n","x_test = l_test[:int(l_test.shape[0]*0.5)]\n","y_test = y_ts[:int(l_test.shape[0]*0.5)]\n","\n","batch_size = 100\n","# Shuffle Data\n","x_train, y_train = shuffle(x_train, y_train, random_state=0)\n","x_test, y_test = shuffle(x_test, y_test, random_state=0)\n","x_val, y_val = shuffle(x_val, y_val, random_state=0)\n","\n","# Split into batches\n","batch_size = 100\n","a = 0\n","b = batch_size\n","data_temp = []\n","data_temp2 = []\n","for i in range(int(x_train.shape[0]/batch_size)):\n","    data_temp.append(x_train[a:b])\n","    data_temp2.append(y_train[a:b])\n","    a += batch_size\n","    b += batch_size\n","x_train = np.asarray(data_temp)\n","y_train = np.asarray(data_temp2)\n","\n","# Print data shape\n","print(\"Data Shape\")\n","print(x_train.shape)\n","print(y_train.shape)\n","print(x_test.shape)\n","print(y_test.shape)\n","print(x_val.shape)\n","print(y_val.shape)"],"metadata":{"id":"Y7Az665u4xrM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Define Models"],"metadata":{"id":"n0ajsDeL4fBt"}},{"cell_type":"code","source":["# As described in the project description \n","# A simple CNN model with seven Conv Blocks and three Feature maps for hidden layers\n","class CNN1(nn.Module):\n","    def __init__(self):\n","        super(CNN1, self).__init__()\n","\n","        self.regressor = nn.Sequential(\n","            nn.Conv2d(1, 3, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(3, 3, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(3, 3, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(3, 3, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(3, 3, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(3, 3, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(3, 2, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(2, 2),\n","            nn.ReLU()\n","        )\n","\n","    def forward(self, x):\n","        x = self.regressor(x)\n","        return x\n","\n","# Hyperparameter tuning\n","# Trying different set of feature maps \n","class CNN2(nn.Module):\n","    def __init__(self):\n","        super(CNN2, self).__init__()\n","\n","        self.regressor = nn.Sequential(\n","            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(256, 64),\n","            nn.Linear(64, 2),\n","            nn.ReLU()\n","        )\n","\n","    def forward(self, x):\n","        x = self.regressor(x)\n","        return x\n","\n","# Increasing the number of feature maps \n","class CNN3(nn.Module):\n","    def __init__(self):\n","        super(CNN3, self).__init__()\n","\n","        self.regressor = nn.Sequential(\n","            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(512, 128),\n","            nn.Linear(128, 2),\n","            nn.ReLU()\n","        )\n","\n","    def forward(self, x):\n","        x = self.regressor(x)\n","        return x\n","\n","# Push model to gpu\n","model = CNN2().to(\"cuda\")"],"metadata":{"id":"tguxlwZH0yoN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Train Model"],"metadata":{"id":"X0zDWkhKkWNO"}},{"cell_type":"code","source":["# Reset Model\n","for layer in model.children():\n","   if hasattr(layer, 'reset_parameters'):\n","       layer.reset_parameters()\n","\n","# Loss Function\n","criteria = torch.nn.MSELoss()\n","\n","# Hyperparameter tuning - Grid search on LR and epochs\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n","n_epochs = 100\n","\n","# Training\n","loss_array = []\n","pbar = tqdm(range(1, n_epochs+1))\n","for epoch in pbar:\n","    train_loss = 0.0\n","    \n","    for i in range(x_train.shape[0]):\n","\n","        data = torch.from_numpy(x_train[i].astype('float32'))\n","        if torch.cuda.is_available():\n","          data = data.cuda()\n","        labels = torch.tensor(y_train[i], dtype=torch.float, device=\"cuda\")\n","        optimizer.zero_grad()\n","        outputs = model(data)\n","        loss = criteria(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss\n","\n","    # Monitor Validation Loss\n","    '''\n","    val_data = torch.from_numpy(x_val.astype('float32'))\n","    if torch.cuda.is_available():\n","      val_data = val_data.cuda()\n","    val_labels = torch.tensor(y_val, dtype=torch.float, device=\"cuda\")\n","    val_outputs = model(val_data)\n","    val_loss = criteria(val_outputs, val_labels)\n","    '''\n","\n","    train_loss_avg = train_loss/x_train.shape[0]\n","    #pbar.set_postfix({ 'Training Loss': train_loss_avg.detach().cpu().numpy(), 'Val Loss': val_loss.detach().cpu().numpy() })  \n","    pbar.set_postfix({ 'Training Loss': train_loss_avg.detach().cpu().numpy() })  \n","\n","# Validation Result for Hyperparameter Search\n","val_data = torch.from_numpy(x_val.astype('float32'))\n","if torch.cuda.is_available():\n","  val_data = val_data.cuda()\n","val_labels = torch.tensor(y_val, dtype=torch.float, device=\"cuda\")\n","val_outputs = model(val_data)\n","val_loss = criteria(val_outputs, val_labels)\n","loss = mean_squared_error(val_outputs.detach().cpu().numpy(), val_labels.cpu().numpy())\n","print(\"Val Result: \" + str(loss))"],"metadata":{"id":"16PwS5P6HaDc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Save Best Model and Test "],"metadata":{"id":"OdHIWPSSmAim"}},{"cell_type":"code","source":["# Reset Model\n","for layer in model.children():\n","   if hasattr(layer, 'reset_parameters'):\n","       layer.reset_parameters()\n","\n","# Model\n","model = CNN2().to(\"cuda\")\n","\n","# Loss Function\n","criteria = torch.nn.MSELoss()\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-5)\n","n_epochs = 500\n","\n","# Training\n","loss_array = []\n","pbar = tqdm(range(1, n_epochs+1))\n","for epoch in pbar:\n","    train_loss = 0.0\n","    \n","    for i in range(x_train.shape[0]):\n","\n","        data = torch.from_numpy(x_train[i].astype('float32'))\n","        if torch.cuda.is_available():\n","          data = data.cuda()\n","        labels = torch.tensor(y_train[i], dtype=torch.float, device=\"cuda\")\n","        optimizer.zero_grad()\n","        outputs = model(data)\n","        loss = criteria(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss\n","\n","    train_loss_avg = train_loss/x_train.shape[0]\n","    loss_array.append(train_loss_avg.detach().cpu().numpy())\n","    pbar.set_postfix({ 'Training Loss': train_loss_avg.detach().cpu().numpy() })  \n","\n","# Export Training Loss for Plot\n","np.save('./Out/CNN_Regressor_train_loss_Faces_ReLU.npy', loss_array)\n","\n","# Export Trained Model for Transfer Learning\n","torch.save(model, './Out/CNN_Regressor_Faces_ReLU.pth')\n","\n","# Testing on Test Data\n","test_data = torch.from_numpy(x_test.astype('float32'))\n","if torch.cuda.is_available():\n","  test_data = test_data.cuda()\n","test_labels = torch.tensor(y_test, dtype=torch.float, device=\"cuda\")\n","test_outputs = model(test_data)\n","a_loss = mean_squared_error(test_outputs[:, 0].detach().cpu().numpy(), test_labels[:, 0].cpu().numpy())\n","b_loss = mean_squared_error(test_outputs[:, 1].detach().cpu().numpy(), test_labels[:, 1].cpu().numpy())\n","print(\"Test a Result: \" + str(a_loss))\n","print(\"Test b Result: \" + str(b_loss))"],"metadata":{"id":"yUqFbOpAmCu4"},"execution_count":null,"outputs":[]}]}